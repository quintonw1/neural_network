# Neural Network - Challenge 19
In the AlphabetSoupChallenge.ipynb file within this repo, a Deep Neual Network with two hidden layers was built to predict the success rate for loans that were provided by AlphabetSoup. An accuracy of 75% and higher was required for the model. A datafile consisting of details for previous loan applicants was used to train and test the model. The data was initially filtered to remove the irrelevant input variables, and then the categorical data was encoded using the OneHotEncoder package from within Sklearn. Various optimzation manipulations were performed and the following observations were made: 
1. Based on the standard reccomended starting points for neurons, an initial value of 2 times the number of input parameters was used. Sensitivity on this number was performed and it was determined that it had little impact on the results. 
2. Upon initial model specifications (Relu, and 2x input variables with a 50% in the second hidden layer), the 75% accuracy was obtained, however there were signs of slight overfitting. Different activation functions were used to try and improve the accuracy and reduce the overfitting, but the inital specifications yielded the best results. 
3. Additional improvement methods were also pursued such as feature elimination, where the companies name (label), was removed from the input data, but after training and testing the model, it was shown to perform worse than previously. 

Reccomendations: 
- A drastic amount of data was bucketed in order to save simulation time. The bucketing criteria was not clearly met, and instead a decision to bucket was made to reduce solve time instead of for statistical reasons. This may have caused issues with the end resulting model, and as a result, if time permits, the entire dataset should be considered instead of bucketing some of the values. 
- As mentioned above, the simulation time when considering all of the initial features or input values was significant and potential model accuracy was sacrificed to manage this issue. A similar machine learning model such as a random forest classifier could be utilized for this example, since all of the data is in tabular form, and the model itself is able to fit itself to the training data much quicker than a neural network. 
